{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38a83f4-4698-4a08-b0d2-31a62b41fc9f",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e7a653-7ac7-4cf3-805f-406a9b2d2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForImageClassification, ViTModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DonutProcessor\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from unids import DonutDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ac11b-50b1-4b48-9ce0-c844a8b55f2e",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69537d47-be14-4871-95a9-c7f93e589916",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/'.join(os.getcwd().split('/')[:3])\n",
    "\n",
    "vit_path = f'{base_dir}/work/models/vit'\n",
    "llama_path = f'{base_dir}/work/models/llama2'\n",
    "mistral_path = f'{base_dir}/work/models/mistral'\n",
    "gemma2_path = f'{base_dir}/work/models/gemma2'\n",
    "unichart_path = f'{base_dir}/work/models/unichart/Encoder'\n",
    "ds_path = f'{base_dir}/uniptds'\n",
    "gemma22_path = f'{base_dir}/work/models/gemma2-2B'\n",
    "\n",
    "image_folder = f'{base_dir}/content/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab393a00-95de-4210-b9d5-b077cb58d365",
   "metadata": {},
   "source": [
    "## Setting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33376665-8ded-4b54-9b57-2bd9e3aeb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb401c3b-1044-4f1b-a8d6-92dcfac1688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274b1e6-4476-4c3d-9652-48ffb9b5b9bd",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a40add-aa3e-467a-a10f-a7481c483204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_processor():\n",
    "    def __init__(self, vision_processor, text_tokenizer):\n",
    "        self.processor = vision_processor\n",
    "        self.tokenizer = text_tokenizer\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image_tensor = vision_transform(image).unsqueeze(0)\n",
    "        return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cf4827-5752-4257-b21c-d6b52296f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentMLP(nn.Module):\n",
    "    def __init__(self, vision_dim, text_dim, hidden_dim):\n",
    "        super(AlignmentMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(vision_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, text_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b914f3-111c-4619-bb78-878e03fbb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, vision_model, alignment_mlp, text_model):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.alignment_mlp = alignment_mlp\n",
    "        self.text_model = text_model\n",
    "\n",
    "    def forward(self, pixel_values, input_ids):\n",
    "        tmp = self.vision_model(pixel_values)\n",
    "        vision_outputs = tmp.last_hidden_state\n",
    "        print(vision_outputs.shape, \" !! VIT \")\n",
    "        aligned_features = self.alignment_mlp(vision_outputs)\n",
    "\n",
    "        text_embeddings = self.text_model.get_input_embeddings()(input_ids)\n",
    "        print(aligned_features.shape, \" @@ aligned \")\n",
    "        print(text_embeddings.shape, \" ## embeddings \")\n",
    "        combined_features = torch.cat((aligned_features, text_embeddings), dim=1)\n",
    "\n",
    "        outputs = self.text_model(inputs_embeds=combined_features)\n",
    "\n",
    "        return outputs.logits\n",
    "        \n",
    "        # print(pixel_values.shape, \" #@#@#! \")\n",
    "        # tmp = self.vision_model(pixel_values)\n",
    "        # vision_outputs = tmp.last_hidden_state[0]\n",
    "        # print(vision_outputs.shape, \" ^^^^^ \")\n",
    "        # aligned_features = self.alignment_mlp(vision_outputs)\n",
    "        # print(aligned_features.shape, \" nomnomnom \")\n",
    "        # text_embeddings = self.text_model.get_input_embeddings()(input_ids)\n",
    "        # combined_features = torch.cat((aligned_features.unsqueeze(0), text_embeddings), dim=1)\n",
    "\n",
    "        # outputs = self.text_model(inputs_embeds=combined_features)\n",
    "        # ######## INCLUDE IF USING TRAINER #############\n",
    "        # # logits = outputs.logits[:,577:]\n",
    "        # # logits = logits.view(-1, 32000)  # Shape: [1024, 128256]\n",
    "        # # labels = labels.view(-1)\n",
    "        # # criterion = nn.CrossEntropyLoss()\n",
    "        # # loss = criterion(logits, labels)\n",
    "        # # #return {\"logits\": outputs.logits, \"loss\": loss}\n",
    "        # # return loss\n",
    "        # #####################################################\n",
    "\n",
    "        # return {\"logits\": outputs.logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d3de8-19cf-40ac-8472-d015079377f4",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8791077-b439-4a50-9f21-3b8a938fce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            JOBID     USER              ACCOUNT           NAME  ST  TIME_LEFT NODES CPUS TRES_PER_N MIN_MEM NODELIST (REASON) \n",
      "         23837177    msm97       ctb-enamul_gpu    interactive   R    4:14:57     1   32 gres:gpu:a    125G gra1361 (None) \n"
     ]
    }
   ],
   "source": [
    "!sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4750c431-9a39-47f1-a4ac-2978ddb30ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at /local/msm97.23837177.0/work/models/vit and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vision_model = ViTModel.from_pretrained(vit_path)\n",
    "vision_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1be3e30-386a-4f08-8f66-4075b1ef2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1024 for LLama, 2048 for gemma\n",
    "alignment_mlp = AlignmentMLP(vision_dim=768, text_dim=2304, hidden_dim=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a212983-2b00-40fb-8051-494c4847ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2dedb9f8624aa09bba25a4d48d8c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text_tokenizer = AutoTokenizer.from_pretrained(llama_path)\n",
    "# text_model = LlamaForCausalLM.from_pretrained(llama_path)\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(gemma22_path)\n",
    "text_model = AutoModelForCausalLM.from_pretrained(gemma22_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccbd7de-8de3-4e48-b1b5-3d07292790d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a29a3ab-1880-4a9f-b952-eb1c0e67eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b01f442c-27dc-45e5-a1e0-b19bfde6155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer.pad_token = text_tokenizer.eos_token\n",
    "text_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9eabd02-3afb-4540-ad13-8cc84226503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = test_processor(vision_transform, text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a581fde1-df04-4b41-be86-3b572708702f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalModel(\n",
       "  (vision_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (alignment_mlp): AlignmentMLP(\n",
       "    (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=4096, out_features=2304, bias=True)\n",
       "  )\n",
       "  (text_model): Gemma2ForCausalLM(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "            (rotary_emb): Gemma2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm()\n",
       "          (post_attention_layernorm): Gemma2RMSNorm()\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_model = MultimodalModel(vision_model, alignment_mlp, text_model)\n",
    "multimodal_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938ac525-f85d-49d5-898f-9af3f1ed92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ad1444-0adc-4769-8ff6-cb3bcfc6a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                Output Shape              Param #        \n",
      "============================================================\n",
      "vision_model.embeddings.cls_token [1, 1, 768]               768            \n",
      "vision_model.embeddings.position_embeddings [1, 577, 768]             443136         \n",
      "vision_model.embeddings.patch_embeddings.projection.weight [768, 3, 16, 16]          589824         \n",
      "vision_model.embeddings.patch_embeddings.projection.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.0.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.0.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.0.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.0.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.0.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.0.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.0.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.0.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.0.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.0.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.1.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.1.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.1.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.1.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.1.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.1.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.1.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.1.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.1.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.1.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.2.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.2.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.2.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.2.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.2.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.2.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.2.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.2.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.2.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.2.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.3.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.3.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.3.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.3.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.3.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.3.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.3.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.3.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.3.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.3.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.4.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.4.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.4.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.4.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.4.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.4.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.4.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.4.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.4.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.4.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.5.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.5.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.5.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.5.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.5.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.5.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.5.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.5.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.5.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.5.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.6.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.6.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.6.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.6.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.6.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.6.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.6.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.6.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.6.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.6.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.7.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.7.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.7.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.7.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.7.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.7.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.7.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.7.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.7.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.7.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.8.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.8.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.8.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.8.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.8.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.8.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.8.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.8.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.8.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.8.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.9.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.9.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.9.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.9.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.9.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.9.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.9.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.9.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.9.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.9.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.10.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.10.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.10.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.10.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.10.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.10.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.10.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.10.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.10.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.10.layernorm_after.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.attention.attention.query.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.11.attention.attention.query.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.attention.attention.key.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.11.attention.attention.key.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.attention.attention.value.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.11.attention.attention.value.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.attention.output.dense.weight [768, 768]                589824         \n",
      "vision_model.encoder.layer.11.attention.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.intermediate.dense.weight [3072, 768]               2359296        \n",
      "vision_model.encoder.layer.11.intermediate.dense.bias [3072]                    3072           \n",
      "vision_model.encoder.layer.11.output.dense.weight [768, 3072]               2359296        \n",
      "vision_model.encoder.layer.11.output.dense.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.layernorm_before.weight [768]                     768            \n",
      "vision_model.encoder.layer.11.layernorm_before.bias [768]                     768            \n",
      "vision_model.encoder.layer.11.layernorm_after.weight [768]                     768            \n",
      "vision_model.encoder.layer.11.layernorm_after.bias [768]                     768            \n",
      "vision_model.layernorm.weight [768]                     768            \n",
      "vision_model.layernorm.bias [768]                     768            \n",
      "vision_model.pooler.dense.weight [768, 768]                589824         \n",
      "vision_model.pooler.dense.bias [768]                     768            \n",
      "alignment_mlp.fc1.weight [4096, 768]               3145728        \n",
      "alignment_mlp.fc1.bias [4096]                    4096           \n",
      "alignment_mlp.fc2.weight [2304, 4096]              9437184        \n",
      "alignment_mlp.fc2.bias [2304]                    2304           \n",
      "text_model.model.embed_tokens.weight [256000, 2304]            589824000      \n",
      "text_model.model.layers.0.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.0.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.0.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.0.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.0.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.0.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.0.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.0.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.0.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.0.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.0.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.1.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.1.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.1.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.1.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.1.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.1.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.1.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.1.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.1.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.1.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.1.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.2.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.2.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.2.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.2.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.2.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.2.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.2.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.2.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.2.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.2.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.2.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.3.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.3.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.3.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.3.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.3.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.3.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.3.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.3.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.3.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.3.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.3.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.4.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.4.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.4.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.4.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.4.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.4.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.4.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.4.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.4.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.4.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.4.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.5.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.5.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.5.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.5.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.5.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.5.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.5.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.5.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.5.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.5.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.5.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.6.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.6.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.6.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.6.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.6.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.6.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.6.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.6.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.6.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.6.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.6.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.7.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.7.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.7.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.7.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.7.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.7.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.7.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.7.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.7.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.7.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.7.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.8.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.8.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.8.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.8.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.8.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.8.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.8.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.8.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.8.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.8.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.8.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.9.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.9.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.9.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.9.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.9.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.9.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.9.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.9.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.9.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.9.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.9.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.10.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.10.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.10.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.10.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.10.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.10.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.10.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.10.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.10.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.10.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.10.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.11.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.11.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.11.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.11.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.11.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.11.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.11.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.11.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.11.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.11.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.11.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.12.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.12.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.12.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.12.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.12.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.12.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.12.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.12.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.12.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.12.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.12.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.13.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.13.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.13.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.13.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.13.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.13.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.13.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.13.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.13.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.13.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.13.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.14.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.14.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.14.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.14.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.14.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.14.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.14.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.14.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.14.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.14.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.14.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.15.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.15.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.15.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.15.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.15.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.15.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.15.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.15.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.15.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.15.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.15.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.16.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.16.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.16.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.16.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.16.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.16.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.16.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.16.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.16.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.16.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.16.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.17.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.17.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.17.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.17.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.17.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.17.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.17.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.17.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.17.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.17.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.17.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.18.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.18.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.18.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.18.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.18.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.18.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.18.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.18.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.18.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.18.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.18.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.19.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.19.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.19.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.19.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.19.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.19.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.19.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.19.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.19.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.19.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.19.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.20.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.20.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.20.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.20.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.20.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.20.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.20.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.20.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.20.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.20.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.20.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.21.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.21.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.21.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.21.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.21.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.21.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.21.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.21.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.21.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.21.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.21.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.22.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.22.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.22.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.22.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.22.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.22.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.22.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.22.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.22.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.22.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.22.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.23.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.23.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.23.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.23.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.23.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.23.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.23.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.23.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.23.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.23.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.23.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.24.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.24.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.24.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.24.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.24.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.24.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.24.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.24.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.24.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.24.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.24.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.25.self_attn.q_proj.weight [2048, 2304]              4718592        \n",
      "text_model.model.layers.25.self_attn.k_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.25.self_attn.v_proj.weight [1024, 2304]              2359296        \n",
      "text_model.model.layers.25.self_attn.o_proj.weight [2304, 2048]              4718592        \n",
      "text_model.model.layers.25.mlp.gate_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.25.mlp.up_proj.weight [9216, 2304]              21233664       \n",
      "text_model.model.layers.25.mlp.down_proj.weight [2304, 9216]              21233664       \n",
      "text_model.model.layers.25.input_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.25.post_attention_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.25.pre_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.layers.25.post_feedforward_layernorm.weight [2304]                    2304           \n",
      "text_model.model.norm.weight [2304]                    2304           \n",
      "============================================================\n",
      "Total Trainable Params: 2713612288\n"
     ]
    }
   ],
   "source": [
    "def print_model_summary(model):\n",
    "    print(f\"{'Layer':<20} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"=\" * 60)\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        param_shape = list(param.shape)\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        print(f\"{name:<20} {str(param_shape):<25} {param_count:<15}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "# Print the model summary\n",
    "print_model_summary(multimodal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c096bff-f5ae-4471-b931-c1088670543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_model_weights(multimodal_model.vision_model)\n",
    "#freeze_model_weights(multimodal_model.text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6198272e-9f3c-4c79-82f7-c756535b74de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model.embeddings.cls_token: False\n",
      "vision_model.embeddings.position_embeddings: False\n",
      "vision_model.embeddings.patch_embeddings.projection.weight: False\n",
      "vision_model.embeddings.patch_embeddings.projection.bias: False\n",
      "vision_model.encoder.layer.0.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.0.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.0.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.0.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.0.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.0.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.0.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.0.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.0.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.0.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.0.output.dense.weight: False\n",
      "vision_model.encoder.layer.0.output.dense.bias: False\n",
      "vision_model.encoder.layer.0.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.0.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.0.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.0.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.1.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.1.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.1.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.1.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.1.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.1.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.1.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.1.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.1.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.1.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.1.output.dense.weight: False\n",
      "vision_model.encoder.layer.1.output.dense.bias: False\n",
      "vision_model.encoder.layer.1.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.1.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.1.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.1.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.2.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.2.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.2.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.2.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.2.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.2.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.2.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.2.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.2.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.2.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.2.output.dense.weight: False\n",
      "vision_model.encoder.layer.2.output.dense.bias: False\n",
      "vision_model.encoder.layer.2.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.2.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.2.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.2.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.3.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.3.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.3.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.3.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.3.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.3.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.3.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.3.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.3.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.3.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.3.output.dense.weight: False\n",
      "vision_model.encoder.layer.3.output.dense.bias: False\n",
      "vision_model.encoder.layer.3.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.3.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.3.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.3.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.4.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.4.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.4.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.4.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.4.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.4.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.4.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.4.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.4.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.4.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.4.output.dense.weight: False\n",
      "vision_model.encoder.layer.4.output.dense.bias: False\n",
      "vision_model.encoder.layer.4.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.4.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.4.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.4.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.5.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.5.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.5.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.5.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.5.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.5.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.5.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.5.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.5.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.5.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.5.output.dense.weight: False\n",
      "vision_model.encoder.layer.5.output.dense.bias: False\n",
      "vision_model.encoder.layer.5.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.5.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.5.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.5.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.6.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.6.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.6.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.6.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.6.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.6.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.6.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.6.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.6.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.6.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.6.output.dense.weight: False\n",
      "vision_model.encoder.layer.6.output.dense.bias: False\n",
      "vision_model.encoder.layer.6.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.6.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.6.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.6.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.7.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.7.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.7.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.7.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.7.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.7.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.7.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.7.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.7.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.7.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.7.output.dense.weight: False\n",
      "vision_model.encoder.layer.7.output.dense.bias: False\n",
      "vision_model.encoder.layer.7.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.7.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.7.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.7.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.8.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.8.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.8.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.8.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.8.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.8.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.8.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.8.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.8.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.8.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.8.output.dense.weight: False\n",
      "vision_model.encoder.layer.8.output.dense.bias: False\n",
      "vision_model.encoder.layer.8.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.8.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.8.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.8.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.9.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.9.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.9.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.9.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.9.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.9.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.9.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.9.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.9.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.9.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.9.output.dense.weight: False\n",
      "vision_model.encoder.layer.9.output.dense.bias: False\n",
      "vision_model.encoder.layer.9.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.9.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.9.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.9.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.10.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.10.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.10.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.10.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.10.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.10.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.10.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.10.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.10.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.10.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.10.output.dense.weight: False\n",
      "vision_model.encoder.layer.10.output.dense.bias: False\n",
      "vision_model.encoder.layer.10.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.10.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.10.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.10.layernorm_after.bias: False\n",
      "vision_model.encoder.layer.11.attention.attention.query.weight: False\n",
      "vision_model.encoder.layer.11.attention.attention.query.bias: False\n",
      "vision_model.encoder.layer.11.attention.attention.key.weight: False\n",
      "vision_model.encoder.layer.11.attention.attention.key.bias: False\n",
      "vision_model.encoder.layer.11.attention.attention.value.weight: False\n",
      "vision_model.encoder.layer.11.attention.attention.value.bias: False\n",
      "vision_model.encoder.layer.11.attention.output.dense.weight: False\n",
      "vision_model.encoder.layer.11.attention.output.dense.bias: False\n",
      "vision_model.encoder.layer.11.intermediate.dense.weight: False\n",
      "vision_model.encoder.layer.11.intermediate.dense.bias: False\n",
      "vision_model.encoder.layer.11.output.dense.weight: False\n",
      "vision_model.encoder.layer.11.output.dense.bias: False\n",
      "vision_model.encoder.layer.11.layernorm_before.weight: False\n",
      "vision_model.encoder.layer.11.layernorm_before.bias: False\n",
      "vision_model.encoder.layer.11.layernorm_after.weight: False\n",
      "vision_model.encoder.layer.11.layernorm_after.bias: False\n",
      "vision_model.layernorm.weight: False\n",
      "vision_model.layernorm.bias: False\n",
      "vision_model.pooler.dense.weight: False\n",
      "vision_model.pooler.dense.bias: False\n",
      "alignment_mlp.fc1.weight: True\n",
      "alignment_mlp.fc1.bias: True\n",
      "alignment_mlp.fc2.weight: True\n",
      "alignment_mlp.fc2.bias: True\n",
      "text_model.model.embed_tokens.weight: True\n",
      "text_model.model.layers.0.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.0.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.0.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.0.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.0.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.0.mlp.up_proj.weight: True\n",
      "text_model.model.layers.0.mlp.down_proj.weight: True\n",
      "text_model.model.layers.0.input_layernorm.weight: True\n",
      "text_model.model.layers.0.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.0.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.0.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.1.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.1.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.1.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.1.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.1.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.1.mlp.up_proj.weight: True\n",
      "text_model.model.layers.1.mlp.down_proj.weight: True\n",
      "text_model.model.layers.1.input_layernorm.weight: True\n",
      "text_model.model.layers.1.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.1.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.1.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.2.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.2.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.2.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.2.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.2.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.2.mlp.up_proj.weight: True\n",
      "text_model.model.layers.2.mlp.down_proj.weight: True\n",
      "text_model.model.layers.2.input_layernorm.weight: True\n",
      "text_model.model.layers.2.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.2.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.2.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.3.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.3.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.3.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.3.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.3.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.3.mlp.up_proj.weight: True\n",
      "text_model.model.layers.3.mlp.down_proj.weight: True\n",
      "text_model.model.layers.3.input_layernorm.weight: True\n",
      "text_model.model.layers.3.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.3.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.3.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.4.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.4.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.4.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.4.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.4.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.4.mlp.up_proj.weight: True\n",
      "text_model.model.layers.4.mlp.down_proj.weight: True\n",
      "text_model.model.layers.4.input_layernorm.weight: True\n",
      "text_model.model.layers.4.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.4.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.4.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.5.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.5.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.5.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.5.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.5.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.5.mlp.up_proj.weight: True\n",
      "text_model.model.layers.5.mlp.down_proj.weight: True\n",
      "text_model.model.layers.5.input_layernorm.weight: True\n",
      "text_model.model.layers.5.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.5.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.5.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.6.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.6.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.6.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.6.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.6.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.6.mlp.up_proj.weight: True\n",
      "text_model.model.layers.6.mlp.down_proj.weight: True\n",
      "text_model.model.layers.6.input_layernorm.weight: True\n",
      "text_model.model.layers.6.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.6.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.6.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.7.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.7.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.7.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.7.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.7.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.7.mlp.up_proj.weight: True\n",
      "text_model.model.layers.7.mlp.down_proj.weight: True\n",
      "text_model.model.layers.7.input_layernorm.weight: True\n",
      "text_model.model.layers.7.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.7.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.7.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.8.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.8.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.8.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.8.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.8.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.8.mlp.up_proj.weight: True\n",
      "text_model.model.layers.8.mlp.down_proj.weight: True\n",
      "text_model.model.layers.8.input_layernorm.weight: True\n",
      "text_model.model.layers.8.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.8.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.8.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.9.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.9.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.9.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.9.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.9.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.9.mlp.up_proj.weight: True\n",
      "text_model.model.layers.9.mlp.down_proj.weight: True\n",
      "text_model.model.layers.9.input_layernorm.weight: True\n",
      "text_model.model.layers.9.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.9.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.9.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.10.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.10.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.10.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.10.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.10.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.10.mlp.up_proj.weight: True\n",
      "text_model.model.layers.10.mlp.down_proj.weight: True\n",
      "text_model.model.layers.10.input_layernorm.weight: True\n",
      "text_model.model.layers.10.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.10.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.10.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.11.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.11.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.11.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.11.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.11.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.11.mlp.up_proj.weight: True\n",
      "text_model.model.layers.11.mlp.down_proj.weight: True\n",
      "text_model.model.layers.11.input_layernorm.weight: True\n",
      "text_model.model.layers.11.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.11.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.11.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.12.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.12.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.12.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.12.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.12.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.12.mlp.up_proj.weight: True\n",
      "text_model.model.layers.12.mlp.down_proj.weight: True\n",
      "text_model.model.layers.12.input_layernorm.weight: True\n",
      "text_model.model.layers.12.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.12.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.12.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.13.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.13.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.13.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.13.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.13.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.13.mlp.up_proj.weight: True\n",
      "text_model.model.layers.13.mlp.down_proj.weight: True\n",
      "text_model.model.layers.13.input_layernorm.weight: True\n",
      "text_model.model.layers.13.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.13.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.13.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.14.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.14.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.14.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.14.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.14.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.14.mlp.up_proj.weight: True\n",
      "text_model.model.layers.14.mlp.down_proj.weight: True\n",
      "text_model.model.layers.14.input_layernorm.weight: True\n",
      "text_model.model.layers.14.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.14.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.14.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.15.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.15.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.15.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.15.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.15.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.15.mlp.up_proj.weight: True\n",
      "text_model.model.layers.15.mlp.down_proj.weight: True\n",
      "text_model.model.layers.15.input_layernorm.weight: True\n",
      "text_model.model.layers.15.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.15.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.15.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.16.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.16.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.16.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.16.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.16.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.16.mlp.up_proj.weight: True\n",
      "text_model.model.layers.16.mlp.down_proj.weight: True\n",
      "text_model.model.layers.16.input_layernorm.weight: True\n",
      "text_model.model.layers.16.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.16.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.16.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.17.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.17.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.17.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.17.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.17.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.17.mlp.up_proj.weight: True\n",
      "text_model.model.layers.17.mlp.down_proj.weight: True\n",
      "text_model.model.layers.17.input_layernorm.weight: True\n",
      "text_model.model.layers.17.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.17.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.17.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.18.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.18.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.18.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.18.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.18.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.18.mlp.up_proj.weight: True\n",
      "text_model.model.layers.18.mlp.down_proj.weight: True\n",
      "text_model.model.layers.18.input_layernorm.weight: True\n",
      "text_model.model.layers.18.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.18.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.18.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.19.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.19.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.19.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.19.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.19.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.19.mlp.up_proj.weight: True\n",
      "text_model.model.layers.19.mlp.down_proj.weight: True\n",
      "text_model.model.layers.19.input_layernorm.weight: True\n",
      "text_model.model.layers.19.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.19.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.19.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.20.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.20.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.20.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.20.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.20.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.20.mlp.up_proj.weight: True\n",
      "text_model.model.layers.20.mlp.down_proj.weight: True\n",
      "text_model.model.layers.20.input_layernorm.weight: True\n",
      "text_model.model.layers.20.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.20.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.20.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.21.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.21.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.21.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.21.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.21.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.21.mlp.up_proj.weight: True\n",
      "text_model.model.layers.21.mlp.down_proj.weight: True\n",
      "text_model.model.layers.21.input_layernorm.weight: True\n",
      "text_model.model.layers.21.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.21.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.21.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.22.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.22.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.22.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.22.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.22.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.22.mlp.up_proj.weight: True\n",
      "text_model.model.layers.22.mlp.down_proj.weight: True\n",
      "text_model.model.layers.22.input_layernorm.weight: True\n",
      "text_model.model.layers.22.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.22.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.22.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.23.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.23.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.23.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.23.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.23.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.23.mlp.up_proj.weight: True\n",
      "text_model.model.layers.23.mlp.down_proj.weight: True\n",
      "text_model.model.layers.23.input_layernorm.weight: True\n",
      "text_model.model.layers.23.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.23.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.23.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.24.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.24.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.24.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.24.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.24.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.24.mlp.up_proj.weight: True\n",
      "text_model.model.layers.24.mlp.down_proj.weight: True\n",
      "text_model.model.layers.24.input_layernorm.weight: True\n",
      "text_model.model.layers.24.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.24.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.24.post_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.25.self_attn.q_proj.weight: True\n",
      "text_model.model.layers.25.self_attn.k_proj.weight: True\n",
      "text_model.model.layers.25.self_attn.v_proj.weight: True\n",
      "text_model.model.layers.25.self_attn.o_proj.weight: True\n",
      "text_model.model.layers.25.mlp.gate_proj.weight: True\n",
      "text_model.model.layers.25.mlp.up_proj.weight: True\n",
      "text_model.model.layers.25.mlp.down_proj.weight: True\n",
      "text_model.model.layers.25.input_layernorm.weight: True\n",
      "text_model.model.layers.25.post_attention_layernorm.weight: True\n",
      "text_model.model.layers.25.pre_feedforward_layernorm.weight: True\n",
      "text_model.model.layers.25.post_feedforward_layernorm.weight: True\n",
      "text_model.model.norm.weight: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in multimodal_model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a778bb9-fe91-4f96-9498-b1eb33429b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  4 13:23:23 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              70W / 300W |  10959MiB / 81920MiB |     13%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              49W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              43W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              47W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     25828      C   ...sm97.23837177.0/work/env/bin/python    10946MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d50d175c-7f01-45e5-9b14-ecfae7f8f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 257419.61 MB\n",
      "Available Memory: 244247.91 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the memory info from /proc/meminfo\n",
    "with open('/proc/meminfo', 'r') as meminfo:\n",
    "    lines = meminfo.readlines()\n",
    "\n",
    "# Extract total and available memory in kB\n",
    "mem_total = int([x for x in lines if 'MemTotal:' in x][0].split()[1])\n",
    "mem_available = int([x for x in lines if 'MemAvailable:' in x][0].split()[1])\n",
    "\n",
    "print(f\"Total Memory: {mem_total / 1024:.2f} MB\")\n",
    "print(f\"Available Memory: {mem_available / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f614c3-572d-4305-a597-d091272ea833",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc7631b1-a09c-4dd7-8ffe-9317cbb59d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Start Index: 2768876\n",
    "# Summarization End Index: 3222856 (Inclusive)\n",
    "\n",
    "# Chart2table Start Index: 5637216\n",
    "\n",
    "#dataset size: 6898333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e796f18-9299-4d40-ad5b-3721c0f247be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m huh_tr \u001b[38;5;241m=\u001b[39m DonutDataset(ds_path, image_folder, \u001b[38;5;241m1024\u001b[39m, \u001b[43mprocessor\u001b[49m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt_end_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s_answer>\u001b[39m\u001b[38;5;124m'\u001b[39m, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;241m6898333\u001b[39m))))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#huh_ev = DonutDataset(ds_path, image_folder, 1024, processor, prompt_end_token = '<s_answer>', indices = list(range(16,20)))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "huh_tr = DonutDataset(ds_path, image_folder, 1024, processor, split = 'train', prompt_end_token = '<s_answer>', indices = list(range(len(6898333))))\n",
    "#huh_ev = DonutDataset(ds_path, image_folder, 1024, processor, prompt_end_token = '<s_answer>', indices = list(range(16,20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2c85793-8408-4cf6-89c4-9542836b246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(huh_tr, batch_size=2, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "078d7cd8-869c-4a19-b6a1-ff8fc48f236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl, evaldl = DataLoader(huh_tr, batch_size=1, shuffle=True), DataLoader(huh_ev, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d4777-2e9a-43f6-8424-ee3dc553f554",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d6783f6-d9a5-4b09-9d76-43939b23ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62612435-7ccb-4d9d-a8cd-b90d6240aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(multimodal_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "360db5df-aa56-4062-bc9a-7a39e3e8fc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  4 13:23:24 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              70W / 300W |  10959MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              71W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              55W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     25828      C   ...sm97.23837177.0/work/env/bin/python    10946MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4a896e4-6c21-4652-a96f-2a23589f62e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     num_train_epochs=5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     warmup_steps=0,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=1,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     # save_strategy=\"epoch\",\n",
    "#     # load_best_model_at_end=True,\n",
    "#     # save_total_limit=3,\n",
    "#     # # Use multiple GPUs\n",
    "#     dataloader_num_workers=4,\n",
    "#     fp16=True,  # Enable 16-bit precision training if applicable\n",
    "#     # report_to=\"none\",\n",
    "#     gradient_accumulation_steps=2,\n",
    "# )\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     print(f\"GPU {i}:\")\n",
    "#     print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "#     print(f\"  Reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    "#     print(f\"  Total: {torch.cuda.get_device_properties(i).total_memory / 1024**2:.2f} MB\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=multimodal_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=huh_tr,\n",
    "#     eval_dataset=huh_ev,\n",
    "# )\n",
    "\n",
    "# # Start training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33028e8d-b09f-43a3-84df-8c76f49ec472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ============ Epoch 1 of 1 ========== \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(2674.6038, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(2536.2090, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(2366.6509, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(2263.1768, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(1867.3826, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(1767.3698, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "torch.Size([2, 1601, 256000])  !!! \n",
      "torch.Size([2048, 256000])  !!!  torch.Size([2048])\n",
      "Loss:  tensor(1976.9526, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([1, 577, 768])  !! VIT \n",
      "torch.Size([1, 577, 2304])  @@ aligned \n",
      "torch.Size([1, 1024, 2304])  ## embeddings \n",
      "torch.Size([1, 1601, 256000])  !!! \n",
      "torch.Size([1024, 256000])  !!!  torch.Size([1024])\n",
      "Loss:  tensor(801.5717, device='cuda:0', grad_fn=<AddBackward0>)  !! \n",
      "torch.Size([2, 577, 768])  !! VIT \n",
      "torch.Size([2, 577, 2304])  @@ aligned \n",
      "torch.Size([2, 1024, 2304])  ## embeddings \n",
      "Validation Loss:  tensor(4.7446, device='cuda:0')  !! \n",
      "torch.Size([1, 577, 768])  !! VIT \n",
      "torch.Size([1, 577, 2304])  @@ aligned \n",
      "torch.Size([1, 1024, 2304])  ## embeddings \n",
      "Validation Loss:  tensor(3.7026, device='cuda:0')  !! \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\" ============ Epoch {epoch+1} of {num_epochs} ========== \")\n",
    "    for batch in dataloader:\n",
    "        #image_input, text_ids, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        pixel_values, input_ids, labels = batch['pixel_values'].to(device), batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        #print(text_ids.shape, \" !!! \", image_input.shape, \" ^^^ \", labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = multimodal_model(pixel_values,input_ids)\n",
    "        print(outputs.shape, \" !!! \")\n",
    "        #loss = criterion(outputs, labels)\n",
    "\n",
    "        logits = outputs[:,577:]\n",
    "        logits = logits.contiguous().view(-1, logits.size(-1))  # Shape: [1024, 128256]\n",
    "        labels = labels.contiguous().view(-1)\n",
    "        print(logits.shape, \" !!! \", labels.shape)\n",
    "        loss = 0\n",
    "        for enumeraterow in \n",
    "        loss = criterion(logits, labels)\n",
    "        print(\"Loss: \", loss, \" !! \")        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    multimodal_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in evaldl:\n",
    "            #image_input, text_ids, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "            # logits = self.forward(pixel_values, input_ids)\n",
    "            # logits = logits[:, 577:]  # logits shape: [4, 1024, 32000]\n",
    "            # logits = logits.contiguous().view(-1, logits.size(-1))  # Reshape to [4 * 1024, 32000]        \n",
    "            # labels = labels.contiguous().view(-1)  # Reshape to [4 * 1024]\n",
    "            # #loss = criterion(logits, labels)      \n",
    "            # loss = self.criterion(logits, labels)\n",
    "        \n",
    "            pixel_values, input_ids, labels = batch['pixel_values'].to(device), batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = multimodal_model(pixel_values,input_ids)\n",
    "            #loss = criterion(outputs['logits'], labels)\n",
    "            logits = outputs[:,577:]\n",
    "            logits = logits.contiguous().view(-1, logits.size(-1))  # Shape: [1024, 128256]\n",
    "            labels = labels.contiguous().view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            print(\"Validation Loss: \", loss, \" !! \")  \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # # Get the predicted class\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            # total += labels.size(0)\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(evaldl)\n",
    "    # val_accuracy = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e02ef-f9f7-413b-ae80-39df58bc8faa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444e062-10dc-4078-8d61-7fa3b182ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the memory info from /proc/meminfo\n",
    "with open('/proc/meminfo', 'r') as meminfo:\n",
    "    lines = meminfo.readlines()\n",
    "\n",
    "# Extract total and available memory in kB\n",
    "mem_total = int([x for x in lines if 'MemTotal:' in x][0].split()[1])\n",
    "mem_available = int([x for x in lines if 'MemAvailable:' in x][0].split()[1])\n",
    "\n",
    "print(f\"Total Memory: {mem_total / 1024:.2f} MB\")\n",
    "print(f\"Available Memory: {mem_available / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dff765-9405-44c0-9e18-e158c36db006",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pl_pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95204e45-262b-4449-96fa-9535f253acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fb90e-54bf-444a-9c99-94d195588a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
